{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Measurements_text-generation-using-an-lstm-in-keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afoteygh/Research/blob/main/Measurements_text_generation_using_an_lstm_in_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "cb992059cbdce9902dd23be8246fc180e6c96aa6",
        "id": "-sf-TL7qYkOE"
      },
      "source": [
        "# Text Generation using an LSTM in Keras\n",
        "In this kernel you we will go over how to let a network create text in the style of sir arthur conan doyle. This kernel is heavily based on the [official keras text generation example](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py).  I also made [a video](https://youtu.be/QtQt1CUEE3w) on text generation using an LSTM network.\n",
        "Content:\n",
        "1. [Introduction](#1)\n",
        "2. [Loading in data](#2)\n",
        "3. [Preprocessing](#3)  \n",
        "    3.1 [Map chars to integers](#3.1)  \n",
        "    3.2 [Split up into subsequences](#3.2)\n",
        "4. [Building model](#4)  \n",
        "    4.1 [Helper Functions](#4.1)  \n",
        "    4.2 [Defining callbacks and training the model](#4.2)\n",
        "5. [Generate new text](#5)  \n",
        "6. [Conclusion](#6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2066f1978370082a41b410a67e440e7ed4cb72c7",
        "id": "cneQdlP0YkOE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "027dbe6d5cefff7d3eac6147ef54a023dcf36d7d",
        "id": "BBg2HIbaYkOF"
      },
      "source": [
        "<a id=\"1\"></a> \n",
        "## 1. Introduction\n",
        "Because the sequence in an text is important we will recurrent neural network which can remember its previous inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "H6Sq_X1FYkOF",
        "outputId": "a5508880-1bfe-4d52-d549-ead1a349362d"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "id": "JMSCs4d0YkOG"
      },
      "source": [
        "<a id=\"2\"></a> \n",
        "## 2. Loading in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e0f294d2dcfa87c33bd3c0906379f44986d1b8d6",
        "id": "JBhO4a-xYkOG",
        "outputId": "e12ab07e-a556-4470-cc2d-c5a6fc9f7e5c"
      },
      "source": [
        "text = open('C:/Users/Administrator/Documents/pyproj/SirEben/kaggle 3/wonderland.txt', 'r').read().lower()\n",
        "print('text length', len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text length 144408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ee0e4330b29978330549ec6c7014e5a0c9496d06",
        "id": "I0TbmPl5YkOG",
        "outputId": "4a89976a-a524-44c3-822f-e853b5c4ad0d"
      },
      "source": [
        "print(text[:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "alice's adventures in wonderland\n",
            "\n",
            "lewis carroll\n",
            "\n",
            "the millennium fulcrum edition 3.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "chapter i. down the rabbit-hole\n",
            "\n",
            "alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\n",
            "book her sister was reading, but it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "144150bd91525a6a2f026162efec60d443008dae",
        "id": "bwbb-oGkYkOH"
      },
      "source": [
        "<a id=\"3\"></a> \n",
        "## 3. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b40a7e3f910b134f8dbce978e13d01d42088b667",
        "id": "vrq1n-2IYkOH"
      },
      "source": [
        "<a id=\"3.1\"></a> \n",
        "### 3.1 Map chars to integers\n",
        "\n",
        "Because we will be training on a character level we need to relate each unique character with a number.\n",
        "We are going to create two dicts one from character to integer and one to transform back to character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2f424b2f58c7039f62b564a1aeceb5b3c5d86901",
        "id": "U2apg9KTYkOH",
        "outputId": "201cd812-60db-44a3-e440-e83da9a36052"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print('total chars: ', len(chars))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars:  45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "402f6e5ce9e8f7cdc2e7dcb55266aad1a8e3bb09",
        "id": "8xYx1MdPYkOH"
      },
      "source": [
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a7a3ab9e3f6ad6e22a53090e50f4d6d5afdc3283",
        "id": "wqIU1U7FYkOH"
      },
      "source": [
        "<a id=\"3.2\"></a> \n",
        "### 3.2 Split up into subsequences\n",
        "Creates an array of sentence data with the length maxlen as well as an array with the next character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1fb13b2a1bd1ea7f801e751c1327ae59efc85a46",
        "id": "jqUIh1wjYkOH",
        "outputId": "105f3435-f2c6-4fbd-c102-8c4d53e7c581"
      },
      "source": [
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 48123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1e627d19acc63ef301ca201de88147afac2990f7",
        "id": "zV-Pw_1iYkOH",
        "outputId": "12f85724-a5bf-455b-bf56-efe088ad5c68"
      },
      "source": [
        "print(sentences[:3])\n",
        "print(next_chars[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"\\nalice's adventures in wonderland\\n\\nlewis\", \"ice's adventures in wonderland\\n\\nlewis ca\", \"'s adventures in wonderland\\n\\nlewis carro\"]\n",
            "[' ', 'r', 'l']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8def15c914a431fe23459e7aa456d9c480b0ec99",
        "id": "Mp4Nmtk1YkOI",
        "outputId": "0bdce6b1-7d9e-4ebe-e2b2-7ea4feaae87b"
      },
      "source": [
        "# Print length\n",
        "print(len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ef6c437ef7a96590ddaa03b28a0d777f9a069e8b",
        "id": "bWn9O05jYkOI"
      },
      "source": [
        "We need to reshape our data in a format we can pass to the Keras LSTM The shape look like [samples, time steps, features]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a730301d1db3f0f64faa5d6461ed6b8baefe72de",
        "id": "5em8KMsWYkOI"
      },
      "source": [
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cef36cdb2cd80069386cb0b86573da6b575591f0",
        "id": "yXX3h3gxYkOI",
        "outputId": "78b69ef5-6aa0-4bb9-94dd-52fa47f95feb"
      },
      "source": [
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ True False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False  True False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False  True False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]]\n",
            "[[False  True False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "   True False False False False False False False False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False  True False False False False False\n",
            "  False False False False False False False False False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0f18f120974528b74cae18b26a2ebff16a9f14a6",
        "id": "tWKLx_BVYkOI"
      },
      "source": [
        "<a id=\"4\"></a> \n",
        "## 4. Building model\n",
        "For this kernel I will use a really small LSTM network but if you want to get better results feel free to replace it with a bigger network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "08cc043586d3804558e0dde91563ac01c58f6a72",
        "id": "GxIZaxuiYkOI",
        "outputId": "d26c923d-e41f-4165-eacf-5e330f20ebfb"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dropout(0.2)) \n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ca6506184b319333e23a9531f69b956c6e8528bc",
        "id": "bQhIJb1lYkOJ",
        "outputId": "d2a18273-7dc5-457d-f214-2ce361786a50"
      },
      "source": [
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy','mae','mse'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5e3029c587f24cd5eb7a20645e907c2b5815ff77",
        "id": "vtJDZCMvYkOJ"
      },
      "source": [
        "<a id=\"4.1\"></a> \n",
        "### 4.1 Helper Functions\n",
        "\n",
        "I got this function from the lstm_text_generation example from keras. [https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ddbf609aff0de89026f9367ab02c20fc5055c0cd",
        "id": "3dLTokcWYkOJ"
      },
      "source": [
        "Samples an index from a probability array with some temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "10de369a6283954c1f5564d2f3d152cdf9f9efe2",
        "id": "qLsTz_7nYkOJ"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e59c92fa7e68491270814a2bd209ece712df4672",
        "id": "uK6PUuSoYkOJ"
      },
      "source": [
        "Callback function to print predicted text generated by our LSTM. It prints generated text with 5 different temperatures [0.2, 0.5, 1.0, 1.2]. 0.2 will generate text with more ordinary word. 1.2 will generate wilder guesses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "993684b063bddea5f291c5e710c852da815e3362",
        "id": "sJuL83KMYkOJ"
      },
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "31701fe51d0d0d7dea6bc8c50b5e135c96ecfa75",
        "id": "cekt8llDYkOJ"
      },
      "source": [
        "<a id=\"4.2\"></a> \n",
        "### 4.2 Defining callbacks and training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ded5a7907c6f5d1b7b7fa4e5c8c6cc445bd4a450",
        "id": "wukkdGQ0YkOJ"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
        "                             verbose=1, save_best_only=True,\n",
        "                             mode='min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "57f9ab71278ec6584dc3f2a4166b292d3d79163d",
        "id": "KBh0nt0VYkOJ"
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=1, min_lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "87f151c81b294538020863d07466c9bebee2c1ea",
        "id": "2gP7Z2TzYkOJ"
      },
      "source": [
        "callbacks = [print_callback, checkpoint, reduce_lr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a0898aff47687be0d20c30bc8ba35ada5a6cc446",
        "id": "beBsAULPYkOJ",
        "outputId": "419f81f8-689a-4d8f-f5e5-dee6eb433b09"
      },
      "source": [
        "model.fit(x, y, batch_size=256, epochs=1, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "48123/48123 [==============================] - 20s 408us/step - loss: 2.5247 - acc: 0.3014 - mean_absolute_error: 0.0371 - mean_squared_error: 0.0185s - loss: 2.5400 - acc: 0.2981 - mean_absolute_error: 0.0373 - mean_squared\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ng to draw,' the dormouse went on, yawni\"\n",
            "ng to draw,' the dormouse went on, yawning the the cout so she she the she the the mume the mout the mutt it a dound the sous to the mous the she the wint home the mome the mout to the she the more the whe some the she the moust the the the she the whe the coute the mout in and the the the mout to the moust it and the moust the mout in a but the mome the the court to the she the wame the the the the the mond the she cout the whot won th\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ng to draw,' the dormouse went on, yawni\"\n",
            "ng to draw,' the dormouse went on, yawning she coure ald the bust it all when and the cour in she wor the mouss to whe the wher she mod at he the matt dong she sart rout the the qued she here the she weed\n",
            "cout sed the whe the cowe her it wis so\n",
            "sed\n",
            "'the sers mound she mongen and wo mous er moug the she whot the more of mourd the whe wer whe to kert whre thing the a the of the muttout to the woond whof in the hams and all and but what it\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ng to draw,' the dormouse went on, yawni\"\n",
            "ng to draw,' the dormouse went on, yawniy shen\n",
            "'g!ish'\n",
            "\n",
            "'\n",
            "\n",
            "'the megtelf.\n",
            "'y mo wirfand the thnelver so kntcevy mom she muatice fot herred buthen'n hic indcheve\n",
            "but'.'\n",
            "\n",
            "''whlin mugrond the then all in ghy weat lice chencen, io tonce,'' i'\n",
            "\n",
            "'in of\n",
            "ank leve of ald wall stely in to gomme fo cuce an, uh kelice foomule. bimcunst.n she rucee riot rof wode 'eble :t hamt molee.\n",
            "\n",
            "'o mang inke, thaid in't tawa\n",
            "conith of the buronsw, whreling dow,'\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ng to draw,' the dormouse went on, yawni\"\n",
            "ng to draw,' the dormouse went on, yawniont hamt,' histg  mucfrey' thouhlata't t' deemf fnc\" of che funt onx 'bwfof.h\n",
            "\"tbumrring\n",
            "hadting and, ink.t ing you fons.t\n",
            "juds sheemy' his il wlbkese\n",
            "f ham!' stomkealtsoquadf.\n",
            "'tamee incet moss.\n",
            "\n",
            "'matre.\n",
            "\n",
            "'a\n",
            "nounxs she.\n",
            "'shen to rame the, garee rerinhelpmot angad pren''\n",
            "\n",
            "'bhout\n",
            "thi's the ! us on pad whe.\n",
            "s\n",
            "uo saloj sou !o whieseffe: of sargay yo 'thifkthe shedan!d gjehtqit quever at\n",
            "cuve' wed ghe\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.52475, saving model to weights.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x279ff4e2788>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py6sE8gMYkOK",
        "outputId": "5f653a1a-adf4-43d0-b41e-5a81e9999eec"
      },
      "source": [
        "testScore = model.evaluate(x,y, verbose=0)\n",
        "print ('\\nTest Scores:  acc={}, mae={}, mse={}'.format(*testScore)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Scores:  acc=2.0600175274515076, mae=0.4020946325097816, mse=0.033119961810506984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "72d813d0114724321428166ffc4b40f2cbb3a809",
        "id": "vM37LK7QYkOK"
      },
      "source": [
        "<a id=\"5\"></a> \n",
        "## 5. Generate new text\n",
        "\n",
        "\n",
        "We can generate text using the same approach as in the on_epoch_end helper function create by Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9d2f2eb1e7fec259e85a982e1cff85a9ee8603e2",
        "id": "Msdg1rRwYkOK"
      },
      "source": [
        "def generate_text(length, diversity):\n",
        "    # Get random starting text\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "              \n",
        "    return generated\n",
        "\n",
        "#testScore = model.evaluate(preds, diversity, verbose=0)\n",
        "#print ('\\nTest Scores:  mse={}, mae={}'.format(*testScore))\n",
        "#testScore = model.evaluate(preds, verbose=0)\n",
        "#print ('\\nTest Scores:  acc={}'.format(*testScore)) \n",
        "#print(\"Accuracy:\",metrics.accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b295df279033867aa825ca903e96744b2fd10d8c",
        "id": "yNUAlZHRYkOK",
        "outputId": "994dcecf-8b94-4f4e-f71f-d2487905cd57"
      },
      "source": [
        "print(generate_text(5000, 0.2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lf, and shouted out, 'you'd better not dout she mout mout so the much the wat in and the mont the whe the mout the mot so mout the mout it she wat in the mong the mome the she won she wore the the mous so the the what the whe wond and the more the the she pore the whe the she the mout the the mout the what the the the the hame the her the the whe the mone the mont the the coust the mome the the mome the the the were the the moust the moust it and the moust the could the moust whe wor her the she the moust the the the mome the mout the mout wor the whe the whe the more she the the the the wher the she the mont the she the the mong she the was the the she she the ware the sout the she the whe she the whe she the sher the the the she the mout the whe the more the the mout the the the the more the mouse the she mout the mome the wore the the the mont the what the mont she she she wat so the she the the the the mong the she the mout the the the couse the she mong the she the the more the mutt in and the the more the cout to she the has she the mouthe the mout the murt the wond the the mome the mong the the mome the and the she sous the the the was the whe her and the she mong the she mome the the mont the cout so she the she hame she the mome the she the wore the wat so the the mont so the mous the the whe the mome the mout to the coume the the the mor the she the wome the she won the sout in the mout the the more the the she mot the mome the the the must the and the mout she she she the what the she whe the monce the she she she the mome the the mong the whe the the the mout the the mome the the she she whe she moust and all the she she the mutt on the the was she mout the mong the whe her the the mout the whe the mame the she what the mout in the the more the mome the the the for the mout the mert the the mome the the mome the the mome the the more the the she the morse the mont the mout in the the mouthe the has sous the the mound the the mont she whe her and the mort of whe the what so she she the the cous the mome the more the the she the wonke the the mome the she the mong the wont the she she the coust the mout the mont and the mutt of the was in the she wat in the she the the murt the mout the mout to the she the mont she whe the more the was she mathe she the monge the she was she she whe she the the mout the she mome the the had the whe was in what and the she won the mout sous the the was it the mome the whe the monge the mout so the mout the mome the mont the more the what and the the the the mome the the mong the the the the whe the coute the she the mert the murt the whe she wont the mout in and the more the the her the whe she whe she whe the she the she mout in the the mous the wont to moust the cout the whe the the the she the the whe wous she wand the the cout so the the more the whing the she the mout whe the butt of the the mome the she the the the matt the the mont she she wong the she mout so the the mome the she the murt the the the mond the the whe the mome the mad the the whe the mort the whe her the the the she the she the she the mong the she she the cher the mout in and the the mont the the she more the wand the was in and and the she the cous the she the whe the mast the she the the the the the whot the the mome the the whe the mout in and the moutt it and the the mout so the the mout so she the whe she the mous the the coutt the the wand the whe the mome the she the mome the she she mout to the she wore the whe the mont in and the mout she the want the the mome the mont so the whe she mome the mont the more she whe she wher she the more the what to the monge the coust and the whe soust the the mome she the the murte the she she she the mout to the whe the must the she mout in the she the whe the mont and the the wat in the what in and the the muthe the she mont the mong the the was the cound the more the coust it she the whe the mome the was the and the she the the mort the cout in the the she the more the much mo the the and and the the coust and the mon the she the wat and the mor the she mous she what in and the she whe the mutt the was sous the what and the whe she the more the coust and the mont the cout so the muct and the mout so she the mot the cound the the mout and the moust the mout sous the she was in the whe she couthe the mout to the the she was the mome she whon the mout the the mome the mout so the she the whe sout in and the the she whe the mongen all the mor the sout in the what and the the she whe the she the the whe wint the the more the whe she the wand the mome the mome the the the wand the mome the the mont the she was the the she souse the the the mome the wont the mont the whe and the was she whe the mond the the mont the mome the she whe the the murt the she the she cooke the the whe the she the mone the whe the more the more the the the mout sous to she whe the the coutt she the mout the the more the whe sout she whe she the wand the mont she mome the wont the she mout so the the mont and and th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcC15wnYkOK"
      },
      "source": [
        "#testScore = model.evaluate(preds, diversity, verbose=0)\n",
        "#print ('\\nTest Scores:  mse={}, mae={}'.format(*testScore))\n",
        "\n",
        "# print(\"##############################################################\")\n",
        "# print(\"Accuracy:\",metrics.accuracy_score(y_test, predictions))\n",
        "# print(\"Kappa Stats:\",metrics.cohen_kappa_score(y_test, predictions))\n",
        "# print(\"Precision:\",metrics.precision_score(y_test, predictions))\n",
        "# print(\"Recall:\",metrics.recall_score(y_test, predictions))\n",
        "# print(\"Mean Absolute Error:\",metrics.mean_absolute_error(y, preds))\n",
        "# print(\"Mean Squared Error:\",metrics.mean_squared_error(diversity, preds))\n",
        "# print(\"F-Measure:\",metrics.recall_score(y_test, predictions))\n",
        "# print(\"##############################################################\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f93a5fa5593130757a2e36409e0cfee68b53b939",
        "id": "mBQ-3MXDYkOK"
      },
      "source": [
        "<a id=\"6\"></a> \n",
        "## 6. Conclusion\n",
        "\n",
        "\n",
        "After 5 epochs our LSTM performed a ok job and I'm more than satisfied with the result.\n",
        "\n",
        "Here are a few things you can change to get better results\n",
        "\n",
        "1. Add more LSTM Layers.\n",
        "2. Use more LSTM Cells.\n",
        "3. Train for more than 5 epochs. (25+)\n",
        "4. Add dropout Layer.\n",
        "5. Play around with the batch-size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzBjMeDzYkOK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}